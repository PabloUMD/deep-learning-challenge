{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d9f4140",
   "metadata": {},
   "source": [
    "# Alphabet Soup Charity Model Training and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8de422",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Load the charity_data.csv into a Pandas DataFrame\n",
    "url = \"https://static.bc-edx.com/data/dl-1-2/m21/lms/starter/charity_data.csv\"\n",
    "application_df = pd.read_csv(url)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "application_df.head()\n",
    "\n",
    "# Drop the EIN and NAME columns\n",
    "application_df = application_df.drop(columns=[\"EIN\", \"NAME\"])\n",
    "\n",
    "# Check the number of unique values in each column\n",
    "unique_values = application_df.nunique()\n",
    "print(unique_values)\n",
    "\n",
    "# For columns with more than 10 unique values, count the occurrences for each unique value\n",
    "for column in application_df.columns:\n",
    "    if application_df[column].nunique() > 10:\n",
    "        print(application_df[column].value_counts())\n",
    "\n",
    "# Group rare categorical variables in 'APPLICATION_TYPE' as 'Other'\n",
    "application_counts = application_df.APPLICATION_TYPE.value_counts()\n",
    "rare_application_types = application_counts[application_counts < 1000].index\n",
    "application_df.APPLICATION_TYPE = application_df.APPLICATION_TYPE.replace(rare_application_types, 'Other')\n",
    "\n",
    "# Check if the grouping was successful\n",
    "print(application_df.APPLICATION_TYPE.value_counts())\n",
    "\n",
    "# Apply the same logic to other categorical variables with rare occurrences\n",
    "# Example for 'CLASSIFICATION'\n",
    "classification_counts = application_df.CLASSIFICATION.value_counts()\n",
    "rare_classifications = classification_counts[classification_counts < 1000].index\n",
    "application_df.CLASSIFICATION = application_df.CLASSIFICATION.replace(rare_classifications, 'Other')\n",
    "\n",
    "# Check if the grouping was successful\n",
    "print(application_df.CLASSIFICATION.value_counts())\n",
    "\n",
    "# Use pd.get_dummies() to encode categorical variables\n",
    "application_df = pd.get_dummies(application_df)\n",
    "\n",
    "# Split the preprocessed data into features (X) and target (y)\n",
    "X = application_df.drop(columns=[\"IS_SUCCESSFUL\"])\n",
    "y = application_df[\"IS_SUCCESSFUL\"]\n",
    "\n",
    "# Split the data into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the training and testing features datasets\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Print the shapes of the datasets to confirm\n",
    "print(X_train_scaled.shape, X_test_scaled.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e30b3f2",
   "metadata": {},
   "source": [
    "## Step 2: Compile, Train, and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d535668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the number of input features\n",
    "input_features = X_train_scaled.shape[1]\n",
    "\n",
    "# Create the Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "model.add(Dense(units=80, activation='relu', input_dim=input_features))\n",
    "\n",
    "# Add a second hidden layer\n",
    "model.add(Dense(units=30, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Check the structure of the model\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define a callback to save the model's weights\n",
    "checkpoint = ModelCheckpoint(filepath=\"AlphabetSoupCharity.h5\", save_weights_only=False, save_freq='epoch', verbose=1)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, callbacks=[checkpoint], validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Model Loss: {loss}, Model Accuracy: {accuracy}\")\n",
    "\n",
    "# Save the model to an HDF5 file\n",
    "model.save(\"AlphabetSoupCharity.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d90933",
   "metadata": {},
   "source": [
    "## Step 3: Optimize the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc31379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# Create a new Sequential model with additional layers and dropout\n",
    "model_optimized = Sequential()\n",
    "\n",
    "# Input layer and first hidden layer with more neurons\n",
    "model_optimized.add(Dense(units=100, activation='relu', input_dim=input_features))\n",
    "\n",
    "# Add a dropout layer to prevent overfitting\n",
    "model_optimized.add(Dropout(0.2))\n",
    "\n",
    "# Second hidden layer with increased neurons\n",
    "model_optimized.add(Dense(units=50, activation='relu'))\n",
    "\n",
    "# Add another dropout layer\n",
    "model_optimized.add(Dropout(0.2))\n",
    "\n",
    "# Third hidden layer\n",
    "model_optimized.add(Dense(units=25, activation='relu'))\n",
    "\n",
    "# Output layer with sigmoid activation for binary classification\n",
    "model_optimized.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Check the structure of the optimized model\n",
    "model_optimized.summary()\n",
    "\n",
    "# Compile the optimized model\n",
    "model_optimized.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define a new callback to save the optimized model's weights\n",
    "checkpoint_optimized = ModelCheckpoint(filepath=\"AlphabetSoupCharity_Optimization.h5\", save_weights_only=False, save_freq='epoch', verbose=1)\n",
    "\n",
    "# Train the optimized model\n",
    "history_optimized = model_optimized.fit(X_train_scaled, y_train, epochs=150, callbacks=[checkpoint_optimized], validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Evaluate the optimized model using the test data\n",
    "loss_optimized, accuracy_optimized = model_optimized.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Optimized Model Loss: {loss_optimized}, Optimized Model Accuracy: {accuracy_optimized}\")\n",
    "\n",
    "# Save the optimized model to an HDF5 file\n",
    "model_optimized.save(\"AlphabetSoupCharity_Optimization.h5\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
